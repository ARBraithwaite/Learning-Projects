{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # English Model\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS # stop words\n",
    "# words to keep\n",
    "spacy_stopwords.remove('not')\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "import sys\n",
    "sys.path.append('scripts/')\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\" When learning data science, you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create list of word tokens\n",
    "    token_list = [token.text for token in doc]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "# nlp.add_pipe(sentencizer, before=\"parser\")\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    doc = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in doc])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS # stop words\n",
    "    doc = tokenize(text)\n",
    "    \n",
    "    # filtering stop words\n",
    "    doc = ' '.join([word for word in doc if word.lower() not in spacy_stopwords])\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_space(text):\n",
    "    doc = tokenize(text)\n",
    "    \n",
    "    doc = [word.strip() for word in doc]\n",
    "    doc = ' '.join([word for word in doc if word != ''])\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(corpus):\n",
    "    \n",
    "    normal_corpus = []\n",
    "    \n",
    "    for doc in corpus:\n",
    "        text = \\\n",
    "        remove_stop_words(\n",
    "            remove_white_space(\n",
    "                lemmatize(\n",
    "                    remove_accented_chars(\n",
    "                        remove_special_characters(\n",
    "                            expand_contractions(doc)\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        normal_corpus.append(text)\n",
    "    \n",
    "    return normal_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn datum science discourage challenge setback failure journey'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(corpus):\n",
    "    named_entities = []\n",
    "    for doc in corpus:\n",
    "        \n",
    "        temp_entity_name = ''\n",
    "        temp_named_entity = None\n",
    "        sentence = nlp(doc)\n",
    "\n",
    "        for word in sentence:\n",
    "            term = word.text \n",
    "            tag = word.ent_type_\n",
    "            \n",
    "            if tag:\n",
    "                temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "                temp_named_entity = (temp_entity_name, tag)\n",
    "                \n",
    "            else:\n",
    "                if temp_named_entity:\n",
    "                    named_entities.append(temp_named_entity)\n",
    "                    temp_entity_name = ''\n",
    "                    temp_named_entity = None\n",
    "                    \n",
    "    entity_frame = pd.DataFrame(named_entities, \n",
    "                                columns=['Entity Name', 'Entity Type'])\n",
    "    return entity_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ = \"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n",
    "\n",
    "At least 285 people have contracted measles in the city since September, mostly in Brooklynâ€™s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n",
    "\n",
    "The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Entity Name</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>up to $ 1,000</td>\n",
       "      <td>four</td>\n",
       "      <td>Williamsburg</td>\n",
       "      <td>September</td>\n",
       "      <td>Orthodox Jews</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Bill de Blasio</td>\n",
       "      <td>At least 285</td>\n",
       "      <td>6 months old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entity Type</th>\n",
       "      <td>DATE</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>GPE</td>\n",
       "      <td>DATE</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>GPE</td>\n",
       "      <td>GPE</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1         2             3          4  \\\n",
       "Entity Name  Tuesday  up to $ 1,000      four  Williamsburg  September   \n",
       "Entity Type     DATE          MONEY  CARDINAL           GPE       DATE   \n",
       "Frequency          2              1         1             1          1   \n",
       "\n",
       "                         5              6         7               8  \\\n",
       "Entity Name  Orthodox Jews  New York City  Brooklyn  Bill de Blasio   \n",
       "Entity Type         PERSON            GPE       GPE          PERSON   \n",
       "Frequency                1              1         1               1   \n",
       "\n",
       "                        9            10  \n",
       "Entity Name  At least 285  6 months old  \n",
       "Entity Type      CARDINAL          DATE  \n",
       "Frequency               1             1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_entities = (get_entities([example_]).groupby(by=['Entity Name', 'Entity Type'])\n",
    "                           .size()\n",
    "                           .sort_values(ascending=False)\n",
    "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "top_entities.T.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "Spacy\n",
    "\n",
    "Text: The original word text.\n",
    "\n",
    "Lemma: The base form of the word.\n",
    "\n",
    "POS: The simple part-of-speech tag.\n",
    "\n",
    "Tag: The detailed part-of-speech tag.\n",
    "\n",
    "Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "\n",
    "Shape: The word shape â€“ capitalization, punctuation, digits.\n",
    "\n",
    "is alpha: Is the token an alpha character?\n",
    "\n",
    "is stop: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apples apple NOUN NNS ROOT Xxxxx True False\n"
     ]
    }
   ],
   "source": [
    "token = nlp('Apples')[0]\n",
    "print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"ad1971962c7c4153912916a87e66ef20-0\" class=\"displacy\" width=\"1625\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">loved</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">movie</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">but</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">food</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">was</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">rubbish</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ad1971962c7c4153912916a87e66ef20-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ad1971962c7c4153912916a87e66ef20-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ad1971962c7c4153912916a87e66ef20-0-1\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ad1971962c7c4153912916a87e66ef20-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ad1971962c7c4153912916a87e66ef20-0-2\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ad1971962c7c4153912916a87e66ef20-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,354.0 L573.0,342.0 557.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ad1971962c7c4153912916a87e66ef20-0-3\" stroke-width=\"2px\" d=\"M245,352.0 C245,89.5 745.0,89.5 745.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ad1971962c7c4153912916a87e66ef20-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,354.0 L753.0,342.0 737.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ad1971962c7c4153912916a87e66ef20-0-4\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ad1971962c7c4153912916a87e66ef20-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ad1971962c7c4153912916a87e66ef20-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ad1971962c7c4153912916a87e66ef20-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ad1971962c7c4153912916a87e66ef20-0-6\" stroke-width=\"2px\" d=\"M245,352.0 C245,2.0 1275.0,2.0 1275.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ad1971962c7c4153912916a87e66ef20-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,354.0 L1283.0,342.0 1267.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-ad1971962c7c4153912916a87e66ef20-0-7\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-ad1971962c7c4153912916a87e66ef20-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1435.0,354.0 L1443.0,342.0 1427.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc = nlp(\"I loved the movie but the food was rubbish\")\n",
    "displacy.serve(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"This is the game the never, ever ends. I picked this game up thinking I wouldn't like it having never played a previous TES game before. Sat in my Library untouched for about a month or two, and finally took the plunge. I was wrong. I was so very, very wrong. This is probably the best purchase I've ever made on steam. Add in the unlimited potential of modding, and it's an adventure that continues forever. I've probably restarted over a hundred times by now with a new character, and still only have beaten Alduin once. ONCE. There's still so much more do to and explore that I'm still discovering quests and areas and little hidden things. I still can't believe how much there is to do and I'm still finding more. At it stands I'll probably wake up one day having lost all touch with reality and actually start seeing the world as Skyrim with how much I've played. And I'm totally ok with that.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I, the movie, the food] nsubj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I liked the movie but the food was rubbish\")\n",
    "chunks = list(doc.noun_chunks)\n",
    "print(chunks, chunks[0].root.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## Frequency-Based Embeddings\n",
    "\n",
    "Count Vector (Bag of Words)\n",
    "\n",
    "TF-IDF Vector\n",
    "\n",
    "Co-Occurrence Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.Series(example)\n",
    "test[1] = example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = normalize_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = tokenize, ngram_range=(2,2))\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = tokenize, ngram_range=(2,2))\n",
    "\n",
    "# X = bow_vector.fit_transform(corpus)\n",
    "# bow_vector.get_feature_names()\n",
    "# X.toarray()\n",
    "# X.shape\n",
    "\n",
    "# X = tfidf_vector.fit_transform(corpus)\n",
    "# tfidf_vector.get_feature_names()\n",
    "# X.toarray()\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def cooccurrence_matrix(corpus, window_size = 1):\n",
    "    vocabulary={}\n",
    "    data=[]\n",
    "    row=[]\n",
    "    col=[]\n",
    "    for sentence in corpus:\n",
    "        sentence = tokenize(sentence)\n",
    "        for pos, token in enumerate(sentence):\n",
    "            i = vocabulary.setdefault(token, len(vocabulary))\n",
    "            start = max(0, pos-window_size)\n",
    "            end = min(len(sentence), pos + (window_size + 1))\n",
    "            for pos2 in range(start, end):\n",
    "                if pos2 == pos: \n",
    "                    continue\n",
    "                j = vocabulary.setdefault(sentence[pos2],len(vocabulary))\n",
    "                \n",
    "                data.append(1.)\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "                \n",
    "    cooccurrence_matrix = csr_matrix((data,(row,col)))\n",
    "    return vocabulary, cooccurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, coo_mat = cooccurrence_matrix(corpus, window_size = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learn</th>\n",
       "      <th>datum</th>\n",
       "      <th>science</th>\n",
       "      <th>not</th>\n",
       "      <th>discourage</th>\n",
       "      <th>challenge</th>\n",
       "      <th>setback</th>\n",
       "      <th>failure</th>\n",
       "      <th>journey</th>\n",
       "      <th>New</th>\n",
       "      <th>...</th>\n",
       "      <th>receive</th>\n",
       "      <th>inoculation</th>\n",
       "      <th>child</th>\n",
       "      <th>young</th>\n",
       "      <th>6</th>\n",
       "      <th>month</th>\n",
       "      <th>old</th>\n",
       "      <th>resist</th>\n",
       "      <th>fine</th>\n",
       "      <th>1000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>learn</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datum</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>science</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discourage</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resist</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fine</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            learn  datum  science  not  discourage  challenge  setback  \\\n",
       "learn         0.0    1.0      1.0  0.0         0.0        0.0      0.0   \n",
       "datum         1.0    0.0      1.0  1.0         0.0        0.0      0.0   \n",
       "science       1.0    1.0      0.0  1.0         1.0        0.0      0.0   \n",
       "not           0.0    1.0      1.0  0.0         1.0        2.0      1.0   \n",
       "discourage    0.0    0.0      1.0  1.0         0.0        1.0      1.0   \n",
       "...           ...    ...      ...  ...         ...        ...      ...   \n",
       "month         0.0    0.0      0.0  0.0         0.0        0.0      0.0   \n",
       "old           0.0    0.0      0.0  0.0         0.0        0.0      0.0   \n",
       "resist        0.0    0.0      0.0  0.0         0.0        0.0      0.0   \n",
       "fine          0.0    0.0      0.0  0.0         0.0        0.0      0.0   \n",
       "1000          0.0    0.0      0.0  0.0         0.0        0.0      0.0   \n",
       "\n",
       "            failure  journey  New  ...  receive  inoculation  child  young  \\\n",
       "learn           0.0      0.0  0.0  ...      0.0          0.0    0.0    0.0   \n",
       "datum           0.0      0.0  0.0  ...      0.0          0.0    0.0    0.0   \n",
       "science         0.0      0.0  0.0  ...      0.0          0.0    0.0    0.0   \n",
       "not             1.0      1.0  0.0  ...      0.0          0.0    0.0    0.0   \n",
       "discourage      0.0      0.0  0.0  ...      0.0          0.0    0.0    0.0   \n",
       "...             ...      ...  ...  ...      ...          ...    ...    ...   \n",
       "month           0.0      0.0  0.0  ...      0.0          0.0    0.0    1.0   \n",
       "old             0.0      0.0  0.0  ...      0.0          0.0    0.0    0.0   \n",
       "resist          0.0      0.0  0.0  ...      0.0          0.0    0.0    0.0   \n",
       "fine            0.0      0.0  0.0  ...      0.0          0.0    0.0    0.0   \n",
       "1000            0.0      0.0  0.0  ...      0.0          0.0    0.0    0.0   \n",
       "\n",
       "              6  month  old  resist  fine  1000  \n",
       "learn       0.0    0.0  0.0     0.0   0.0   0.0  \n",
       "datum       0.0    0.0  0.0     0.0   0.0   0.0  \n",
       "science     0.0    0.0  0.0     0.0   0.0   0.0  \n",
       "not         0.0    0.0  0.0     0.0   0.0   0.0  \n",
       "discourage  0.0    0.0  0.0     0.0   0.0   0.0  \n",
       "...         ...    ...  ...     ...   ...   ...  \n",
       "month       1.0    0.0  1.0     1.0   0.0   0.0  \n",
       "old         1.0    1.0  0.0     1.0   1.0   0.0  \n",
       "resist      0.0    1.0  1.0     0.0   1.0   1.0  \n",
       "fine        0.0    0.0  1.0     1.0   0.0   1.0  \n",
       "1000        0.0    0.0  0.0     1.0   1.0   0.0  \n",
       "\n",
       "[64 rows x 64 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(coo_mat.toarray(), index = vocab.keys(), columns = vocab.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Word Count of the documents â€“ total number of words in the documents\n",
    "\n",
    "Character Count of the documents â€“ total number of characters in the documents\n",
    "\n",
    "Average Word Density of the documents â€“ average length of the words used in the documents\n",
    "\n",
    "Puncutation Count in the Complete Essay â€“ total number of punctuation marks in the documents\n",
    "\n",
    "Upper Case Count in the Complete Essay â€“ total number of upper count words in the documents\n",
    "\n",
    "Title Word Count in the Complete Essay â€“ total number of proper case (title) words in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "data['char_count'] = data['text'].apply(len)\n",
    "# Character Count\n",
    "data['word_count'] = data['text'].apply(lambda x: len(tokenize(x)))\n",
    "# Punctuation count\n",
    "data['punctuation_count'] = data['text'].apply(lambda x: count_punct(x))\n",
    "# Upper Case count\n",
    "data['upper_case_word_count'] = data['text'].apply(lambda x: count_upper(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def count_punct(text):\n",
    "    doc = tokenize(text)\n",
    "    \n",
    "    punctuation = [punct for punct in doc if punct in string.punctuation]\n",
    "    \n",
    "    return len(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_upper(text):\n",
    "    doc = tokenize(text)\n",
    "    \n",
    "    upper  =[word for word in doc if word.isupper()]\n",
    "    \n",
    "    return len(upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency distribution of Part of Speech Tags:\n",
    "Noun Count\n",
    "\n",
    "Verb Count\n",
    "\n",
    "Adjective Count\n",
    "\n",
    "Adverb Count\n",
    "\n",
    "Pronoun Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_type = {\n",
    "    'adjective': 'ADJ',\n",
    "    'adverb': 'ADV',\n",
    "    'verb': 'VERB',\n",
    "    'pronoun': 'PRON',\n",
    "    'noun': 'NOUN',\n",
    "    \n",
    "}\n",
    "\n",
    "def count_pos_type(text, pos: str):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    if pos in pos_type.keys():\n",
    "        pos_tagged = [word.pos_ for word in doc if word.pos_ == pos_type[pos]]\n",
    "        \n",
    "        return len(pos_tagged)\n",
    "    else:\n",
    "        return f'{pos} not in  accepted pos types {pos_type.keys()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pos_type(example, 'verb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['noun_count'] = data['text'].apply(lambda text: count_pos_type(text, 'noun'))\n",
    "data['verb_count'] = data['text'].apply(lambda text: count_pos_type(text, 'verb'))\n",
    "data['adj_count'] = data['text'].apply(lambda text: count_pos_type(text, 'adjective'))\n",
    "data['adv_count'] = data['text'].apply(lambda text: count_pos_type(text, 'adverb'))\n",
    "data['pron_count'] = data['text'].apply(lambda text: count_pos_type(text, 'pronoun'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
