{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\") # English Model\n",
    "\n",
    "example = \"\"\" When learning data science,   you shouldn't get discouraged!\n",
    "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Create list of word tokens\n",
    "    token_list = [token.text for token in doc]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentencizer = nlp.create_pipe(\"sentencizer\")\n",
    "# nlp.add_pipe(sentencizer, before=\"parser\")\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sentences = [sent for sent in doc.sents]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    doc = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in doc])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    \n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS # stop words\n",
    "    # words to keep\n",
    "    spacy_stopwords.remove('not')\n",
    "\n",
    "    doc = tokenize(text)\n",
    "    \n",
    "    # filtering stop words\n",
    "    doc = ' '.join([word for word in doc if word.lower() not in spacy_stopwords])\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_white_space(text):\n",
    "    doc = tokenize(text)\n",
    "    \n",
    "    doc = [word.strip() for word in doc]\n",
    "    doc = ' '.join([word for word in doc if word != ''])\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(corpus):\n",
    "    \n",
    "    normal_corpus = []\n",
    "    \n",
    "    for doc in corpus:\n",
    "        text = \\\n",
    "        remove_stop_words(\n",
    "            remove_white_space(\n",
    "                lemmatize(\n",
    "                    remove_accented_chars(\n",
    "                        remove_special_characters(\n",
    "                            expand_contractions(doc)\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "        normal_corpus.append(text)\n",
    "    \n",
    "    return normal_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn datum science discourage challenge setback failure journey'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(corpus):\n",
    "    named_entities = []\n",
    "    for doc in corpus:\n",
    "        \n",
    "        temp_entity_name = ''\n",
    "        temp_named_entity = None\n",
    "        sentence = nlp(doc)\n",
    "\n",
    "        for word in sentence:\n",
    "            term = word.text \n",
    "            tag = word.ent_type_\n",
    "            \n",
    "            if tag:\n",
    "                temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "                temp_named_entity = (temp_entity_name, tag)\n",
    "                \n",
    "            else:\n",
    "                if temp_named_entity:\n",
    "                    named_entities.append(temp_named_entity)\n",
    "                    temp_entity_name = ''\n",
    "                    temp_named_entity = None\n",
    "                    \n",
    "    entity_frame = pd.DataFrame(named_entities, \n",
    "                                columns=['Entity Name', 'Entity Type'])\n",
    "    return entity_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_ = \"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n",
    "\n",
    "At least 285 people have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n",
    "\n",
    "The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Entity Name</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>up to $ 1,000</td>\n",
       "      <td>four</td>\n",
       "      <td>Williamsburg</td>\n",
       "      <td>September</td>\n",
       "      <td>Orthodox Jews</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Bill de Blasio</td>\n",
       "      <td>At least 285</td>\n",
       "      <td>6 months old</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entity Type</th>\n",
       "      <td>DATE</td>\n",
       "      <td>MONEY</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>GPE</td>\n",
       "      <td>DATE</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>GPE</td>\n",
       "      <td>GPE</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>DATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Frequency</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0              1         2             3          4  \\\n",
       "Entity Name  Tuesday  up to $ 1,000      four  Williamsburg  September   \n",
       "Entity Type     DATE          MONEY  CARDINAL           GPE       DATE   \n",
       "Frequency          2              1         1             1          1   \n",
       "\n",
       "                         5              6         7               8  \\\n",
       "Entity Name  Orthodox Jews  New York City  Brooklyn  Bill de Blasio   \n",
       "Entity Type         PERSON            GPE       GPE          PERSON   \n",
       "Frequency                1              1         1               1   \n",
       "\n",
       "                        9            10  \n",
       "Entity Name  At least 285  6 months old  \n",
       "Entity Type      CARDINAL          DATE  \n",
       "Frequency               1             1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_entities = (get_entities([example_]).groupby(by=['Entity Name', 'Entity Type'])\n",
    "                           .size()\n",
    "                           .sort_values(ascending=False)\n",
    "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "top_entities.T.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings\n",
    "\n",
    "## Frequency-Based Embeddings\n",
    "\n",
    "Count Vector (Bag of Words)\n",
    "\n",
    "TF-IDF Vector\n",
    "\n",
    "Co-Occurrence Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learn datum science not discourage challenge setback not failure journey',\n",
       " 'New York City Tuesday declare public health emergency order mandatory measle vaccination amid outbreak late national flash point refusal inoculate dangerous disease 285 people contract measle city September Brooklyns Williamsburg neighborhood order cover Zip code Mayor Bill de Blasio D Tuesday mandate order unvaccinate people area include concentration Orthodox Jews receive inoculation include child young 6 month old resist fine 1000']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.Series(example)\n",
    "test[1] = example_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = normalize_text(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = tokenize, ngram_range=(2,2))\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = tokenize, ngram_range=(2,2))\n",
    "\n",
    "# X = bow_vector.fit_transform(corpus)\n",
    "# bow_vector.get_feature_names()\n",
    "# X.toarray()\n",
    "# X.shape\n",
    "\n",
    "# X = tfidf_vector.fit_transform(corpus)\n",
    "# tfidf_vector.get_feature_names()\n",
    "# X.toarray()\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def cooccurrence_matrix(corpus, window_size = 1):\n",
    "    vocabulary={}\n",
    "    data=[]\n",
    "    row=[]\n",
    "    col=[]\n",
    "    for sentence in corpus:\n",
    "        sentence = tokenize(sentence)\n",
    "        for pos, token in enumerate(sentence):\n",
    "            i = vocabulary.setdefault(token, len(vocabulary))\n",
    "            start = max(0, pos-window_size)\n",
    "            end = min(len(sentence), pos + (window_size + 1))\n",
    "            for pos2 in range(start, end):\n",
    "                if pos2 == pos: \n",
    "                    continue\n",
    "                j = vocabulary.setdefault(sentence[pos2],len(vocabulary))\n",
    "                \n",
    "                data.append(1.)\n",
    "                row.append(i)\n",
    "                col.append(j)\n",
    "                \n",
    "    cooccurrence_matrix = csr_matrix((data,(row,col)))\n",
    "    return vocabulary, cooccurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [normalize_text(example)]\n",
    "vocab, coo_mat = cooccurrence_matrix(corpus, window_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Word Count of the documents – total number of words in the documents\n",
    "\n",
    "Character Count of the documents – total number of characters in the documents\n",
    "\n",
    "Average Word Density of the documents – average length of the words used in the documents\n",
    "\n",
    "Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
    "\n",
    "Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
    "\n",
    "Title Word Count in the Complete Essay – total number of proper case (title) words in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "data['char_count'] = data['text'].apply(len)\n",
    "# Character Count\n",
    "data['word_count'] = data['text'].apply(lambda x: len(tokenize(x)))\n",
    "# Punctuation count\n",
    "data['punctuation_count'] = data['text'].apply(lambda x: count_punct(x))\n",
    "# Upper Case count\n",
    "data['upper_case_word_count'] = data['text'].apply(lambda x: count_upper(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def count_punct(text):\n",
    "    doc = tokenize(text)\n",
    "    \n",
    "    punctuation = [punct for punct in doc if punct in string.punctuation]\n",
    "    \n",
    "    return len(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_upper(text):\n",
    "    doc = tokenize(text)\n",
    "    \n",
    "    upper  =[word for word in doc if word.isupper()]\n",
    "    \n",
    "    return len(upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency distribution of Part of Speech Tags:\n",
    "Noun Count\n",
    "\n",
    "Verb Count\n",
    "\n",
    "Adjective Count\n",
    "\n",
    "Adverb Count\n",
    "\n",
    "Pronoun Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_type = {\n",
    "    'adjective': 'ADJ',\n",
    "    'adverb': 'ADV',\n",
    "    'verb': 'VERB',\n",
    "    'pronoun': 'PRON',\n",
    "    'noun': 'NOUN',\n",
    "    \n",
    "}\n",
    "\n",
    "def count_pos_type(text, pos: str):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    if pos in pos_type.keys():\n",
    "        pos_tagged = [word.pos_ for word in doc if word.pos_ == pos_type[pos]]\n",
    "        \n",
    "        return len(pos_tagged)\n",
    "    else:\n",
    "        return f'{pos} not in  accepted pos types {pos_type.keys()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_pos_type(example, 'verb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['noun_count'] = data['text'].apply(lambda text: count_pos_type(text, 'noun'))\n",
    "data['verb_count'] = data['text'].apply(lambda text: count_pos_type(text, 'verb'))\n",
    "data['adj_count'] = data['text'].apply(lambda text: count_pos_type(text, 'adjective'))\n",
    "data['adv_count'] = data['text'].apply(lambda text: count_pos_type(text, 'adverb'))\n",
    "data['pron_count'] = data['text'].apply(lambda text: count_pos_type(text, 'pronoun'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
